big
12notj
Help: Performance analysis for macros on big data tables

Hello,
a clients of mine has this situation I'm analyzing and trying to solve: they have a remote server with a database they need to use, this db has some data which is generated by users filling a form on their website.

Now they need that data to perform analysis with Excel macros in their offices. They thought to download the data they need from the remote db and run the macros on their pcs but the db set is too big (300Mb, 200k rows) and it crashes, obviously. 

I am thinking about setting up a local server with SQL so that the data can be stored on this local server and the client can access to that data through Excel connecting to that server. 

The question is: storing the data on the server but still running the macro locally would make it faster? 

Developing a tailor made web solution would be better in this case?

The macro function is about data slicing (for example , show me the rows where field "A" is more than 1) and they run Office 2007.

Thanks


-------------
Doing your slicing directly with SQL is almost always better.

    SELECT * from table where FieldA > 1

Putting it on a local server might make it a bit faster each run, but again, if you're pulling the minimum amount of data required, shouldn't be too bad.

-------------
Having the data stored locally will only speed up read/write times. If that's the part you're worried about, then yes.

Otherwise, everything will be stored in RAM or in pages, which are both accessed locally.

If they have a web server that's colocated with the database server, and that server is a decent piece of hardware, then it very well may be faster to do all of the processing there by using some sort of web development, provided that your strength in that environment is suitably strong.

Also, depending on the database software, it may be possible to create stored procedures that can preprocess or aggregate some of the data for you, to cut down on data transfer and sample size.
-------------
Indeed, you should do everything you can to minimize I/O, which is the most resource expensive operation. Network communication may be slow, but can be mitigated by minimal data size.
-------------
Yes you undestood the scenario. I am proficient in web development but very short on time for this client.

About the preprocessing procedures you were talking about: is that something that exist in Excel macros too?

At the moment the solution for this case seems to be using only 15k / 30k rows in an Excel file and running local macros.
-------------
In this worksheet I have both full visualization of the data and macros running and calculating parameters all over the data, so I guess the I/O time would be too much and still make the pc crash or be very slow/unresponsive.
-------------
>About the preprocessing procedures you were talking about: is that something that exist in Excel macros too?

No, it's something that would need to be written for the database software and stored in the database. They're like procedural scripts (called stored procedures). The benefit is that they can work with the data VERY quickly and return only what you need.

If it turns out you need the specific detail of each of the 15k-30k rows, then stick with what you have. If you need only an AGGREGATION of that data, in any sort of grouping set, consider re-writing the query or getting a stored procedure on the database to only return the result set needed. The name of the game here is to minimize data transfer. Even if you have to do some post-processing after you get the data, as long as you still get only as many rows as you need, you're sitting pretty.
